//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-30033411
// Cuda compilation tools, release 11.4, V11.4.48
// Based on NVVM 7.0.1
//

.version 7.4
.target sm_50
.address_size 64

	// .globl	__miss__radiance_envtexture
.const .align 8 .b8 params[288];

.visible .entry __miss__radiance_envtexture()
{
	.reg .pred 	%p<16>;
	.reg .f32 	%f<129>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<5>;


	mov.u32 	%r9, 0;
	// begin inline asm
	call (%r8), _optix_get_payload, (%r9);
	// end inline asm
	mov.u32 	%r11, 1;
	// begin inline asm
	call (%r10), _optix_get_payload, (%r11);
	// end inline asm
	cvt.u64.u32 	%rd2, %r8;
	cvt.u64.u32 	%rd3, %r10;
	bfi.b64 	%rd1, %rd2, %rd3, 32, 32;
	ld.u32 	%r1, [%rd1+12];
	and.b32  	%r12, %r1, 4096;
	setp.eq.s32 	%p1, %r12, 0;
	@%p1 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_1;

$L__BB0_4:
	ld.f32 	%f50, [%rd1+128];
	ld.f32 	%f6, [%rd1+132];
	ld.f32 	%f51, [%rd1+136];
	ld.f32 	%f52, [%rd1+96];
	fma.rn.ftz.f32 	%f53, %f50, 0f5A0E1BCA, %f52;
	st.f32 	[%rd1+96], %f53;
	ld.f32 	%f54, [%rd1+100];
	fma.rn.ftz.f32 	%f55, %f6, 0f5A0E1BCA, %f54;
	st.f32 	[%rd1+100], %f55;
	ld.f32 	%f56, [%rd1+104];
	fma.rn.ftz.f32 	%f57, %f51, 0f5A0E1BCA, %f56;
	st.f32 	[%rd1+104], %f57;
	or.b32  	%r18, %r1, -2147483648;
	st.u32 	[%rd1+12], %r18;
	abs.ftz.f32 	%f7, %f50;
	abs.ftz.f32 	%f8, %f51;
	setp.eq.ftz.f32 	%p3, %f7, 0f00000000;
	setp.eq.ftz.f32 	%p4, %f8, 0f00000000;
	and.pred  	%p5, %p3, %p4;
	mov.b32 	%r4, %f50;
	mov.b32 	%r19, %f51;
	and.b32  	%r5, %r19, -2147483648;
	@%p5 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_5;

$L__BB0_8:
	shr.s32 	%r24, %r4, 31;
	and.b32  	%r25, %r24, 1078530011;
	or.b32  	%r26, %r25, %r5;
	mov.b32 	%f126, %r26;
	bra.uni 	$L__BB0_9;

$L__BB0_1:
	ld.u32 	%r13, [%rd1+28];
	mad.lo.s32 	%r14, %r13, 1664525, 1013904223;
	and.b32  	%r15, %r14, 16777215;
	cvt.rn.f32.u32 	%f18, %r15;
	mov.f32 	%f19, 0f4B800000;
	div.approx.ftz.f32 	%f20, %f18, %f19;
	mad.lo.s32 	%r2, %r14, 1664525, 1013904223;
	st.u32 	[%rd1+28], %r2;
	add.ftz.f32 	%f21, %f20, %f20;
	mov.f32 	%f22, 0f3F800000;
	sub.ftz.f32 	%f23, %f22, %f21;
	st.f32 	[%rd1+136], %f23;
	mul.ftz.f32 	%f24, %f23, %f23;
	sub.ftz.f32 	%f1, %f22, %f24;
	setp.leu.ftz.f32 	%p2, %f1, 0f00000000;
	mov.f32 	%f125, 0f00000000;
	@%p2 bra 	$L__BB0_3;

	sqrt.approx.ftz.f32 	%f125, %f1;

$L__BB0_3:
	and.b32  	%r16, %r2, 16777215;
	cvt.rn.f32.u32 	%f25, %r16;
	div.approx.ftz.f32 	%f27, %f25, %f19;
	add.ftz.f32 	%f28, %f27, %f27;
	mul.ftz.f32 	%f29, %f28, 0f40490FDB;
	cos.approx.ftz.f32 	%f30, %f29;
	mul.ftz.f32 	%f31, %f125, %f30;
	st.f32 	[%rd1+128], %f31;
	sin.approx.ftz.f32 	%f32, %f29;
	mul.ftz.f32 	%f33, %f125, %f32;
	st.f32 	[%rd1+132], %f33;
	ld.f32 	%f34, [%rd1+80];
	ld.f32 	%f35, [%rd1+92];
	ld.f32 	%f36, [%rd1+84];
	ld.f32 	%f37, [%rd1+88];
	ld.f32 	%f38, [%rd1];
	fma.rn.ftz.f32 	%f39, %f35, %f34, %f38;
	st.f32 	[%rd1], %f39;
	ld.f32 	%f40, [%rd1+4];
	fma.rn.ftz.f32 	%f128, %f35, %f36, %f40;
	st.f32 	[%rd1+4], %f128;
	ld.f32 	%f41, [%rd1+8];
	fma.rn.ftz.f32 	%f127, %f35, %f37, %f41;
	st.f32 	[%rd1+8], %f127;
	ld.u32 	%r17, [%rd1+12];
	or.b32  	%r32, %r17, 514;
	st.u32 	[%rd1+12], %r32;
	ld.v4.f32 	{%f42, %f43, %f44, %f45}, [%rd1+80];
	st.v4.f32 	[%rd1+48], {%f42, %f43, %f44, %f22};
	bra.uni 	$L__BB0_10;

$L__BB0_5:
	setp.eq.ftz.f32 	%p6, %f7, 0f7F800000;
	setp.eq.ftz.f32 	%p7, %f8, 0f7F800000;
	and.pred  	%p8, %p6, %p7;
	@%p8 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_6;

$L__BB0_7:
	setp.lt.s32 	%p12, %r4, 0;
	selp.b32 	%r22, 1075235812, 1061752795, %p12;
	or.b32  	%r23, %r22, %r5;
	mov.b32 	%f126, %r23;
	bra.uni 	$L__BB0_9;

$L__BB0_6:
	setp.lt.s32 	%p9, %r4, 0;
	min.ftz.f32 	%f58, %f8, %f7;
	max.ftz.f32 	%f59, %f8, %f7;
	div.full.ftz.f32 	%f60, %f58, %f59;
	mul.rn.ftz.f32 	%f61, %f60, %f60;
	mov.f32 	%f62, 0fC0B59883;
	mov.f32 	%f63, 0fBF52C7EA;
	fma.rn.ftz.f32 	%f64, %f61, %f63, %f62;
	mov.f32 	%f65, 0fC0D21907;
	fma.rn.ftz.f32 	%f66, %f64, %f61, %f65;
	mul.ftz.f32 	%f67, %f61, %f66;
	mul.ftz.f32 	%f68, %f60, %f67;
	add.ftz.f32 	%f69, %f61, 0f41355DC0;
	mov.f32 	%f70, 0f41E6BD60;
	fma.rn.ftz.f32 	%f71, %f69, %f61, %f70;
	mov.f32 	%f72, 0f419D92C8;
	fma.rn.ftz.f32 	%f73, %f71, %f61, %f72;
	rcp.approx.ftz.f32 	%f74, %f73;
	fma.rn.ftz.f32 	%f75, %f68, %f74, %f60;
	mov.f32 	%f76, 0f3FC90FDB;
	sub.ftz.f32 	%f77, %f76, %f75;
	setp.gt.ftz.f32 	%p10, %f8, %f7;
	selp.f32 	%f78, %f77, %f75, %p10;
	mov.f32 	%f79, 0f40490FDB;
	sub.ftz.f32 	%f80, %f79, %f78;
	selp.f32 	%f81, %f80, %f78, %p9;
	mov.b32 	%r20, %f81;
	or.b32  	%r21, %r5, %r20;
	mov.b32 	%f82, %r21;
	add.ftz.f32 	%f83, %f7, %f8;
	setp.le.ftz.f32 	%p11, %f83, 0f7F800000;
	selp.f32 	%f126, %f82, %f83, %p11;

$L__BB0_9:
	fma.rn.ftz.f32 	%f84, %f126, 0f3E22F983, 0f3F000000;
	abs.ftz.f32 	%f85, %f6;
	mov.f32 	%f86, 0f3F800000;
	sub.ftz.f32 	%f87, %f86, %f85;
	mul.ftz.f32 	%f88, %f87, 0f3F000000;
	sqrt.approx.ftz.f32 	%f89, %f88;
	setp.gt.ftz.f32 	%p13, %f85, 0f3F11EB85;
	selp.f32 	%f90, %f89, %f85, %p13;
	mul.ftz.f32 	%f91, %f90, %f90;
	mov.f32 	%f92, 0f3C94D2E9;
	mov.f32 	%f93, 0f3D53F941;
	fma.rn.ftz.f32 	%f94, %f93, %f91, %f92;
	mov.f32 	%f95, 0f3D3F841F;
	fma.rn.ftz.f32 	%f96, %f94, %f91, %f95;
	mov.f32 	%f97, 0f3D994929;
	fma.rn.ftz.f32 	%f98, %f96, %f91, %f97;
	mov.f32 	%f99, 0f3E2AAB94;
	fma.rn.ftz.f32 	%f100, %f98, %f91, %f99;
	mul.ftz.f32 	%f101, %f91, %f100;
	fma.rn.ftz.f32 	%f102, %f101, %f90, %f90;
	mov.f32 	%f103, 0f3FC90FDB;
	mov.f32 	%f104, 0fC0000000;
	fma.rn.ftz.f32 	%f105, %f104, %f102, %f103;
	selp.f32 	%f106, %f105, %f102, %p13;
	setp.le.ftz.f32 	%p14, %f106, 0f7F800000;
	mov.b32 	%r27, %f106;
	mov.b32 	%r28, %f6;
	and.b32  	%r29, %r28, -2147483648;
	or.b32  	%r30, %r29, %r27;
	mov.b32 	%f107, %r30;
	selp.f32 	%f108, %f107, %f106, %p14;
	fma.rn.ftz.f32 	%f109, %f108, 0fBEA2F983, 0f3F000000;
	ld.const.u64 	%rd4, [params+224];
	tex.2d.v4.f32.f32 	{%f110, %f111, %f112, %f113}, [%rd4, {%f84, %f109}];
	ld.f32 	%f114, [%rd1];
	add.ftz.f32 	%f115, %f114, %f110;
	st.f32 	[%rd1], %f115;
	ld.f32 	%f116, [%rd1+4];
	add.ftz.f32 	%f128, %f116, %f111;
	st.f32 	[%rd1+4], %f128;
	ld.f32 	%f117, [%rd1+8];
	add.ftz.f32 	%f127, %f117, %f112;
	st.f32 	[%rd1+8], %f127;
	ld.u32 	%r32, [%rd1+12];

$L__BB0_10:
	and.b32  	%r31, %r32, 16777216;
	setp.eq.s32 	%p15, %r31, 0;
	@%p15 bra 	$L__BB0_12;

	ld.f32 	%f118, [%rd1+16];
	ld.f32 	%f119, [%rd1];
	mul.ftz.f32 	%f120, %f119, %f118;
	st.f32 	[%rd1+16], %f120;
	ld.f32 	%f121, [%rd1+20];
	mul.ftz.f32 	%f122, %f128, %f121;
	st.f32 	[%rd1+20], %f122;
	ld.f32 	%f123, [%rd1+24];
	mul.ftz.f32 	%f124, %f127, %f123;
	st.f32 	[%rd1+24], %f124;

$L__BB0_12:
	ret;

}

